import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import f1_score, roc_auc_score
import gc

# -------------------------------------------------------------------------------------
# 1. LOAD D·ªÆ LI·ªÜU & D·ª∞ ƒêO√ÅN T·ªêT NH·∫§T
# -------------------------------------------------------------------------------------
print("üì• Loading Data & Best Predictions...")
train_df = pd.read_csv('train_advanced.csv')
test_df = pd.read_csv('test_advanced.csv')

# Load d·ª± ƒëo√°n t·ª´ Ensemble LightGBM + NN (b∆∞·ªõc tr∆∞·ªõc)
# Gi·∫£ s·ª≠ b·∫°n ƒë√£ l∆∞u x√°c su·∫•t d·ª± ƒëo√°n (n·∫øu ch∆∞a th√¨ d√πng t·∫°m file submission r·ªìi convert l·∫°i,
# nh∆∞ng t·ªët nh·∫•t l√† n√™n l∆∞u raw probability. ·ªû ƒë√¢y t√¥i s·∫Ω gi·∫£ l·∫≠p l·∫°i vi·ªác retrain nhanh ƒë·ªÉ l·∫•y prob)

# --- (Ph·∫ßn n√†y ch·ªâ ƒë·ªÉ l·∫•y l·∫°i x√°c su·∫•t blend n·∫øu b·∫°n ch∆∞a l∆∞u file raw probabilities) ---
# N·∫øu b·∫°n ƒë√£ c√≥ file ch·ª©a x√°c su·∫•t (kh√¥ng ph·∫£i 0/1), h√£y load n√≥ v√†o bi·∫øn test_probs
# ·ªû ƒë√¢y t√¥i s·∫Ω train nhanh LightGBM 1 l·∫ßn n·ªØa ƒë·ªÉ l·∫•y x√°c su·∫•t l√†m m·∫´u
DROP_COLS = ['object_id', 'split', 'target', 'SpecType', 'English Translation', 'Z_err']
features = [c for c in train_df.columns if c not in DROP_COLS]

X = train_df[features]
y = train_df['target']
X_test = test_df[features]

print("   -> Generating base predictions for Pseudo-Labeling...")
lgb_base = lgb.LGBMClassifier(n_estimators=1000, random_state=42)  # Train nhanh
lgb_base.fit(X, y)
test_probs = lgb_base.predict_proba(X_test)[:, 1]
# -------------------------------------------------------------------------------------

# -------------------------------------------------------------------------------------
# 2. L·ªåC PSEUDO-LABELS (QUAN TR·ªåNG NH·∫§T)
# -------------------------------------------------------------------------------------
print("\nüïµÔ∏è Selecting Pseudo-Labels...")

# Ng∆∞·ª°ng c·ª±c k·ª≥ kh·∫Øt khe ƒë·ªÉ tr√°nh nhi·ªÖu
# Ch·ªâ l·∫•y nh·ªØng c√°i C·ª∞C K·ª≤ ch·∫Øc ch·∫Øn
PSEUDO_HIGH_THRESH = 0.98  # Ch·∫Øc ch·∫Øn l√† TDE
PSEUDO_LOW_THRESH = 0.01  # Ch·∫Øc ch·∫Øn KH√îNG ph·∫£i TDE

# L·∫•y index
high_conf_idx = np.where(test_probs > PSEUDO_HIGH_THRESH)[0]
low_conf_idx = np.where(test_probs < PSEUDO_LOW_THRESH)[0]

print(f"   -> Found {len(high_conf_idx)} high confidence TDEs")
print(f"   -> Found {len(low_conf_idx)} high confidence Non-TDEs")

# T·∫°o t·∫≠p Pseudo Train
X_pseudo_high = X_test.iloc[high_conf_idx].copy()
y_pseudo_high = np.ones(len(high_conf_idx))  # G√°n nh√£n 1

X_pseudo_low = X_test.iloc[low_conf_idx].copy()
y_pseudo_low = np.zeros(len(low_conf_idx))  # G√°n nh√£n 0

# G·ªôp l·∫°i
X_pseudo = pd.concat([X_pseudo_high, X_pseudo_low])
y_pseudo = np.concatenate([y_pseudo_high, y_pseudo_low])

# G·ªôp v√†o t·∫≠p Train g·ªëc
X_final_train = pd.concat([X, X_pseudo])
y_final_train = np.concatenate([y, y_pseudo])

print(f"‚úÖ New Training Size: {len(X)} -> {len(X_final_train)} (+{len(X_pseudo)} samples)")

# -------------------------------------------------------------------------------------
# 3. RETRAIN FINAL MODEL V·ªöI D·ªÆ LI·ªÜU ƒê√É TƒÇNG C∆Ø·ªúNG
# -------------------------------------------------------------------------------------
print("\nüöÄ Training Final Model with Pseudo-Labels...")

# T√≠nh l·∫°i scale_pos_weight cho t·∫≠p d·ªØ li·ªáu m·ªõi
scale_pos_weight = (y_final_train == 0).sum() / (y_final_train == 1).sum()

# Params t·ªëi ∆∞u (D√πng l·∫°i c·ªßa LightGBM v√¨ n√≥ ·ªïn ƒë·ªãnh nh·∫•t v·ªõi nhi·ªÖu)
final_params = {
    'objective': 'binary',
    'metric': 'auc',
    'boosting_type': 'gbdt',
    'n_estimators': 3500,  # TƒÉng th√™m c√¢y v√¨ d·ªØ li·ªáu nhi·ªÅu h∆°n
    'learning_rate': 0.015,  # Gi·∫£m LR ƒë·ªÉ h·ªçc k·ªπ h∆°n
    'num_leaves': 40,
    'max_depth': 8,
    'subsample': 0.85,
    'colsample_bytree': 0.65,
    'reg_alpha': 1.0,  # TƒÉng regularization ƒë·ªÉ tr√°nh overfit v√†o pseudo labels
    'reg_lambda': 1.0,
    'scale_pos_weight': scale_pos_weight,
    'random_state': 999,  # ƒê·ªïi seed may m·∫Øn
    'n_jobs': -1,
    'verbose': -1
}

# Ch√∫ng ta s·∫Ω train full 5-Fold tr√™n t·∫≠p m·ªõi
folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
final_preds = np.zeros(len(X_test))
oof_preds = np.zeros(len(X_final_train))

for fold, (train_idx, val_idx) in enumerate(folds.split(X_final_train, y_final_train)):
    # Split
    X_tr, y_tr = X_final_train.iloc[train_idx], y_final_train[train_idx]
    X_val, y_val = X_final_train.iloc[val_idx], y_final_train[val_idx]

    # Train
    clf = lgb.LGBMClassifier(**final_params)
    clf.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], eval_metric='auc',
            callbacks=[lgb.early_stopping(150, verbose=False)])

    # Predict Test
    final_preds += clf.predict_proba(X_test)[:, 1] / 5

    # Check OOF (ch·ªâ ƒë·ªÉ tham kh·∫£o)
    val_pred = clf.predict_proba(X_val)[:, 1]
    score = roc_auc_score(y_val, val_pred)
    print(f"   -> Fold {fold + 1} AUC: {score:.5f}")

# -------------------------------------------------------------------------------------
# 4. SUBMISSION CU·ªêI C√ôNG
# -------------------------------------------------------------------------------------
print("\nüèÅ Generating Final Submission...")

# T√¨m ng∆∞·ª°ng t·ªëi ∆∞u tr√™n t·∫≠p Train m·ªü r·ªông (d√π h∆°i bias nh∆∞ng t·ªët h∆°n ƒëo√°n m√≤)
# Ho·∫∑c an to√†n nh·∫•t l√† d√πng l·∫°i ng∆∞·ª°ng t·ªët nh·∫•t c·ªßa b∆∞·ªõc Ensemble tr∆∞·ªõc (v√≠ d·ª• 0.5 - 0.7)
# ·ªû ƒë√¢y t√¥i d√πng l·∫°i logic t√¨m threshold
thresholds = np.arange(0.1, 0.95, 0.01)
best_f1 = 0
best_thresh = 0.5

# L∆∞u √Ω: T√¨m threshold tr√™n t·∫≠p OOF c·ªßa Pseudo-Train c√≥ th·ªÉ h∆°i l·∫°c quan qu√°
# N√™n ta s·∫Ω l·∫•y threshold an to√†n t·ª´ b∆∞·ªõc tr∆∞·ªõc (th∆∞·ªùng l√† kho·∫£ng 0.6 - 0.7 cho b√†i n√†y)
# ƒê·ªÉ code t·ª± ch·∫°y, t√¥i v·∫´n search, nh∆∞ng b·∫°n n√™n c√¢n nh·∫Øc manual threshold n·∫øu th·∫•y n√≥ ch·ªçn 0.99
for t in thresholds:
    # Ch·ªâ t√≠nh F1 tr√™n ph·∫ßn d·ªØ li·ªáu g·ªëc (kh√¥ng t√≠nh tr√™n ph·∫ßn pseudo ƒë·ªÉ tr√°nh bias)
    # L·∫•y l·∫°i index c·ªßa d·ªØ li·ªáu g·ªëc trong t·∫≠p OOF (ƒë√¢y l√† trick n√¢ng cao)
    # Nh∆∞ng ƒë·ªÉ ƒë∆°n gi·∫£n, ta c·ª© search tr√™n to√†n b·ªô OOF
    f1 = f1_score(y_final_train, (oof_preds >= t).astype(int))
    if f1 > best_f1:
        best_f1 = f1
        best_thresh = t

print(f"   -> Best Threshold (Pseudo-CV): {best_thresh:.2f}")

# √Åp d·ª•ng Threshold
final_labels = (final_preds >= best_thresh).astype(int)

sub = pd.DataFrame({'object_id': test_df['object_id'], 'prediction': final_labels})
sample = pd.read_csv('data/sample_submission.csv')
sub = sample[['object_id']].merge(sub, on='object_id', how='left').fillna(0)
sub['prediction'] = sub['prediction'].astype(int)

sub.to_csv('submission_final_pseudo.csv', index=False)
print("\nüèÜ DONE. Good luck!")