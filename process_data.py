import pandas as pd
import numpy as np
import os
import glob
from scipy.stats import skew, kurtosis
from tqdm import tqdm
import warnings
import gc

# Cáº¥u hÃ¬nh hiá»ƒn thá»‹ vÃ  táº¯t cáº£nh bÃ¡o
warnings.filterwarnings('ignore')
pd.set_option('display.max_columns', None)

# -------------------------------------------------------------------------------------
# 1. Cáº¤U HÃŒNH Há»† THá»NG & Háº°NG Sá» Váº¬T LÃ
# -------------------------------------------------------------------------------------
BASE_PATH = 'data'  # ÄÆ°á»ng dáº«n tá»›i thÆ° má»¥c chá»©a dá»¯ liá»‡u báº¡n Ä‘Ã£ upload
TRAIN_LOG_PATH = os.path.join(BASE_PATH, 'train_log.csv')
TEST_LOG_PATH = os.path.join(BASE_PATH, 'test_log.csv')

# Há»‡ sá»‘ dáº­p táº¯t (Extinction coefficients) R_lambda xáº¥p xá»‰ cho cÃ¡c band cá»§a LSST
# Dá»±a trÃªn Schlafly & Finkbeiner (2011) cho R_V = 3.1
EXTINCTION_COEFFS = {
    'u': 4.81,
    'g': 3.64,
    'r': 2.70,
    'i': 2.06,
    'z': 1.58,
    'y': 1.31
}

# Mapping tÃªn band sang sá»‘ Ä‘á»ƒ xá»­ lÃ½ matrix nhanh hÆ¡n náº¿u cáº§n
BAND_MAP = {'u': 0, 'g': 1, 'r': 2, 'i': 3, 'z': 4, 'y': 5}


# -------------------------------------------------------------------------------------
# 2. HÃ€M Xá»¬ LÃ Váº¬T LÃ & TIá»€N Xá»¬ LÃ (PHYSICS & PREPROCESSING)
# -------------------------------------------------------------------------------------

def correct_flux(df_lc, df_meta):
    """
    Thá»±c hiá»‡n De-extinction cho Flux dá»±a trÃªn EBV.
    CÃ´ng thá»©c: Flux_corr = Flux_obs * 10^(0.4 * A_lambda * EBV)
    Trong Ä‘Ã³ A_lambda = Coeff_band
    """
    # Merge EBV vÃ o lightcurve
    df_lc = df_lc.merge(df_meta[['object_id', 'EBV']], on='object_id', how='left')

    # Táº¡o cá»™t há»‡ sá»‘ R tÆ°Æ¡ng á»©ng vá»›i band
    df_lc['R_factor'] = df_lc['Filter'].map(EXTINCTION_COEFFS)

    # TÃ­nh Flux Ä‘Ã£ hiá»‡u chá»‰nh
    # LÆ°u Ã½: Flux gá»‘c cÃ³ thá»ƒ Ã¢m, viá»‡c nhÃ¢n há»‡ sá»‘ dÆ°Æ¡ng khÃ´ng lÃ m thay Ä‘á»•i dáº¥u
    correction_factor = 10 ** (0.4 * df_lc['R_factor'] * df_lc['EBV'])
    df_lc['Flux_corr'] = df_lc['Flux'] * correction_factor

    # TÃ­nh láº¡i sai sá»‘ Flux (Flux Error cÅ©ng bá»‹ scale tÆ°Æ¡ng á»©ng)
    df_lc['Flux_err_corr'] = df_lc['Flux_err'] * correction_factor

    return df_lc.drop(columns=['EBV', 'R_factor'])


# -------------------------------------------------------------------------------------
# 3. CORE FEATURE ENGINEERING (TRÃCH XUáº¤T Äáº¶C TRÆ¯NG)
# -------------------------------------------------------------------------------------

def extract_features_group(group):
    """
    HÃ m nÃ y xá»­ lÃ½ má»™t nhÃ³m (má»™t object_id) vÃ  tráº£ vá» má»™t Series cÃ¡c Ä‘áº·c trÆ°ng.
    Tuy nhiÃªn, Ä‘á»ƒ tá»‘i Æ°u tá»‘c Ä‘á»™, chÃºng ta sáº½ dÃ¹ng Aggregation cá»§a Pandas thay vÃ¬ apply tá»«ng dÃ²ng.
    HÃ m nÃ y chá»‰ dÃ¹ng Ä‘á»ƒ minh há»a logic náº¿u cáº§n debug.
    ChÃºng ta sáº½ dÃ¹ng vectorization á»Ÿ hÃ m main_extraction bÃªn dÆ°á»›i.
    """
    pass


def aggregate_features(df_lc):
    """
    TÃ­nh toÃ¡n cÃ¡c Ä‘áº·c trÆ°ng thá»‘ng kÃª cho tá»«ng object, tá»«ng band.
    Input: DataFrame Lightcurves (Ä‘Ã£ correct flux)
    Output: DataFrame Features (aggregated)
    """

    # 1. CÃ¡c Ä‘áº·c trÆ°ng cÆ¡ báº£n theo tá»«ng Filter
    aggs = {
        'Flux_corr': ['min', 'max', 'mean', 'median', 'std'],
        'Flux_err_corr': ['mean'],
        'Time (MJD)': ['min', 'max', 'count']  # count lÃ  sá»‘ lÆ°á»£ng quan sÃ¡t
    }

    # Group by Object vÃ  Filter
    features_per_band = df_lc.groupby(['object_id', 'Filter']).agg(aggs)

    # LÃ m pháº³ng MultiIndex columns
    features_per_band.columns = ['_'.join(col).strip() for col in features_per_band.columns.values]
    features_per_band = features_per_band.reset_index()

    # Pivot table Ä‘á»ƒ Ä‘Æ°a Filter lÃªn thÃ nh cá»™t (vÃ­ dá»¥: u_Flux_mean, g_Flux_mean...)
    features_wide = features_per_band.pivot(index='object_id', columns='Filter')

    # LÃ m pháº³ng láº¡i cá»™t sau khi pivot
    features_wide.columns = [f"{col[1]}_{col[0]}" for col in features_wide.columns]

    # 2. TÃ­nh toÃ¡n thÃªm cÃ¡c Ä‘áº·c trÆ°ng phá»©c táº¡p hÆ¡n (Vectorized)
    # Skew & Kurtosis (Cáº§n cáº©n tháº­n vá»›i sá»‘ lÆ°á»£ng máº«u Ã­t)
    skew_kurt = df_lc.groupby(['object_id', 'Filter'])['Flux_corr'].agg(
        skew=lambda x: skew(x, nan_policy='omit') if len(x) > 2 else 0,
        kurt=lambda x: kurtosis(x, nan_policy='omit') if len(x) > 2 else 0
    ).reset_index().pivot(index='object_id', columns='Filter')
    skew_kurt.columns = [f"{col[1]}_Flux_{col[0]}" for col in skew_kurt.columns]

    # Merge láº¡i
    final_features = pd.concat([features_wide, skew_kurt], axis=1)

    return final_features


def calculate_advanced_features(features_df):
    """
    TÃ­nh toÃ¡n cÃ¡c Ä‘áº·c trÆ°ng káº¿t há»£p giá»¯a cÃ¡c bands (Colors, Ratios)
    Dá»±a trÃªn DataFrame Ä‘Ã£ pivot (wide format).
    """
    bands = ['u', 'g', 'r', 'i', 'z', 'y']

    # 1. Colors (Hiá»‡u Ä‘á»™ sÃ¡ng giá»¯a cÃ¡c band liá»n ká» - Ä‘áº¡i diá»‡n nhiá»‡t Ä‘á»™)
    # DÃ¹ng Flux_mean hoáº·c Flux_max Ä‘á»ƒ tÃ­nh color
    for i in range(len(bands) - 1):
        b1 = bands[i]
        b2 = bands[i + 1]
        # Color dá»±a trÃªn Mean Flux
        features_df[f'{b1}_{b2}_flux_diff_mean'] = features_df[f'{b1}_Flux_corr_mean'] - features_df[
            f'{b2}_Flux_corr_mean']
        # Color dá»±a trÃªn Max Flux (Ä‘á»‰nh cá»§a sá»± kiá»‡n)
        features_df[f'{b1}_{b2}_flux_diff_max'] = features_df[f'{b1}_Flux_corr_max'] - features_df[
            f'{b2}_Flux_corr_max']

    # 2. Amplitude (BiÃªn Ä‘á»™ dao Ä‘á»™ng)
    for b in bands:
        if f'{b}_Flux_corr_max' in features_df.columns:
            features_df[f'{b}_amplitude'] = features_df[f'{b}_Flux_corr_max'] - features_df[f'{b}_Flux_corr_min']

    # 3. Global Time Features (Thá»i gian quan sÃ¡t)
    # Láº¥y max(Time_max) - min(Time_min) trÃªn táº¥t cáº£ cÃ¡c band
    time_max_cols = [c for c in features_df.columns if 'Time (MJD)_max' in c]
    time_min_cols = [c for c in features_df.columns if 'Time (MJD)_min' in c]

    # VÃ¬ má»—i band cÃ³ thá»ƒ quan sÃ¡t thá»i Ä‘iá»ƒm khÃ¡c nhau, ta láº¥y min/max tá»•ng thá»ƒ
    # Fillna Ä‘á»ƒ trÃ¡nh lá»—i náº¿u object thiáº¿u band
    features_df['global_start_time'] = features_df[time_min_cols].min(axis=1)
    features_df['global_end_time'] = features_df[time_max_cols].max(axis=1)
    features_df['duration'] = features_df['global_end_time'] - features_df['global_start_time']

    return features_df


# -------------------------------------------------------------------------------------
# 4. PIPELINE CHÃNH (MAIN EXECUTION)
# -------------------------------------------------------------------------------------

def process_dataset(log_path, is_train=True):
    print(f"ğŸ”„ Äang xá»­ lÃ½ táº­p dá»¯ liá»‡u: {'TRAIN' if is_train else 'TEST'}...")

    # Load Metadata
    df_log = pd.read_csv(log_path)

    # Láº¥y danh sÃ¡ch cÃ¡c splits duy nháº¥t
    unique_splits = df_log['split'].unique()

    all_features_list = []

    # Duyá»‡t qua tá»«ng split folder Ä‘á»ƒ Ä‘á»c lightcurves (Tiáº¿t kiá»‡m RAM)
    pbar = tqdm(unique_splits)
    for split_name in pbar:
        pbar.set_description(f"Processing {split_name}")

        # ÄÆ°á»ng dáº«n tá»›i file lightcurve cá»§a split nÃ y
        lc_path = os.path.join(BASE_PATH, split_name,
                               'train_full_lightcurves.csv' if is_train else 'test_full_lightcurves.csv')

        if not os.path.exists(lc_path):
            print(f"âš ï¸ KhÃ´ng tÃ¬m tháº¥y file: {lc_path}, bá» qua.")
            continue

        # Äá»c file LC
        df_lc_split = pd.read_csv(lc_path)

        # Láº¥y metadata tÆ°Æ¡ng á»©ng vá»›i cÃ¡c object trong split nÃ y Ä‘á»ƒ De-extinct
        objects_in_split = df_log[df_log['split'] == split_name]

        # Tiá»n xá»­ lÃ½ Váº­t lÃ½ (De-extinction)
        df_lc_split = correct_flux(df_lc_split, objects_in_split)

        # TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng thá»‘ng kÃª (Aggregation)
        split_features = aggregate_features(df_lc_split)

        # Gom káº¿t quáº£
        all_features_list.append(split_features)

        # Dá»n dáº¹p RAM
        del df_lc_split
        gc.collect()

    # Ná»‘i táº¥t cáº£ cÃ¡c splits
    full_features_df = pd.concat(all_features_list, axis=0)

    # TÃ­nh cÃ¡c Ä‘áº·c trÆ°ng nÃ¢ng cao (Advanced Features)
    full_features_df = calculate_advanced_features(full_features_df)

    # Merge láº¡i vá»›i Metadata gá»‘c (Z, SpecType, target...)
    # LÆ°u Ã½: Index cá»§a full_features_df Ä‘ang lÃ  object_id
    final_df = df_log.merge(full_features_df, on='object_id', how='left')

    return final_df


# -------------------------------------------------------------------------------------
# 5. CHáº Y PIPELINE
# -------------------------------------------------------------------------------------

if __name__ == "__main__":
    # Xá»­ lÃ½ táº­p Train
    if os.path.exists(TRAIN_LOG_PATH):
        train_df = process_dataset(TRAIN_LOG_PATH, is_train=True)
        print(f"âœ… ÄÃ£ xá»­ lÃ½ xong Train Set. Shape: {train_df.shape}")

        # LÆ°u ra CSV Ä‘á»ƒ dÃ¹ng cho cÃ¡c bÆ°á»›c modeling sau
        train_df.to_csv('processed/processed_train_features.csv', index=False)
        print("ğŸ’¾ ÄÃ£ lÆ°u file: processed/processed_train_features.csv")
    else:
        print("âŒ KhÃ´ng tÃ¬m tháº¥y train_log.csv")

    # Xá»­ lÃ½ táº­p Test
    if os.path.exists(TEST_LOG_PATH):
        test_df = process_dataset(TEST_LOG_PATH, is_train=False)
        print(f"âœ… ÄÃ£ xá»­ lÃ½ xong Test Set. Shape: {test_df.shape}")

        # LÆ°u ra CSV
        test_df.to_csv('processed/processed_test_features.csv', index=False)
        print("ğŸ’¾ ÄÃ£ lÆ°u file: processed/processed_test_features.csv")
    else:
        print("âŒ KhÃ´ng tÃ¬m tháº¥y test_log.csv")